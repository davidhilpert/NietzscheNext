{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JcvEz43u3Z8"
      },
      "source": [
        "# Nietzsche Next\n",
        "- description: Autocomplete app trained on Nietzsche's works. Goal is to make autocompletion proposals for the next word based on the wider context and the characters typed in so far. Uses RNN for character-based prediction (sequence-to-sequence). Updated for TPU usage, parallel data processing, and optimized training on Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_6QE_WUylsd",
        "outputId": "57488e03-8003-46d9-9142-250378888317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU not found. Check runtime settings.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"TPU successfully initialized.\")\n",
        "except ValueError:\n",
        "    print(\"TPU not found. Check runtime settings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0asFbfJpulKM",
        "outputId": "c5ff118f-36af-4ee6-ff52-6adf27c33346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFDnxoBavSCR"
      },
      "outputs": [],
      "source": [
        "# 1. Load and preprocess data\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/nietzsche_texts'  # Update this path for your Colab Drive location\n",
        "nietzsche_works_train = \"\"\n",
        "nietzsche_works_test = \"\"\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\") and filename != 'nietzsche_zarathustra_clean.txt':\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            tmp_file = file.read() + \"\\n\"\n",
        "            twothirds = int(len(tmp_file) * 2/3)\n",
        "            train = tmp_file[:twothirds]\n",
        "            test = tmp_file[twothirds:]\n",
        "            nietzsche_works_train += train\n",
        "            nietzsche_works_test += test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K7vx8TBv3sR"
      },
      "outputs": [],
      "source": [
        "def simplify_text(text):\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    text = ''.join([char for char in text if not unicodedata.combining(char)])\n",
        "\n",
        "    replacements = {\n",
        "        '’': \"'\", '‘': \"'\", '“': '\"', '”': '\"', '—': '-',\n",
        "        '–': '-', ';': ',', ':': ',', '§': '', 'Æ': 'AE', 'æ': 'ae',\n",
        "        'Œ': 'OE', 'œ': 'oe'\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        text = text.replace(old, new)\n",
        "\n",
        "    text = re.sub(r'[0-9]', '', text)\n",
        "    text = re.sub(r'[\\n]', ' ', text)\n",
        "\n",
        "    greek_to_latin = {\n",
        "        'α': 'a', 'ά': 'a', 'β': 'b', 'γ': 'g', 'δ': 'd', 'ε': 'e',\n",
        "        'έ': 'e', 'ζ': 'z', 'η': 'h', 'θ': 'th', 'ι': 'i', 'κ': 'k',\n",
        "        'λ': 'l', 'μ': 'm', 'ν': 'n', 'ξ': 'x', 'ο': 'o', 'π': 'p',\n",
        "        'ρ': 'r', 'ς': 's', 'σ': 's', 'τ': 't', 'υ': 'y', 'φ': 'f',\n",
        "        'χ': 'ch', 'ω': 'o', 'ό': 'o'\n",
        "    }\n",
        "    for greek, latin in greek_to_latin.items():\n",
        "        text = text.replace(greek, latin)\n",
        "\n",
        "    allowed_chars = re.compile(r'[A-Za-z .,\\'\"!?-]')\n",
        "    text = ''.join([char for char in text if allowed_chars.match(char)])\n",
        "\n",
        "    return text\n",
        "\n",
        "nietzsche_train_clean = simplify_text(nietzsche_works_train)\n",
        "nietzsche_test_clean = simplify_text(nietzsche_works_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByvTnRibu5X1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# create character mappings\n",
        "chars = sorted(list(set(nietzsche_train_clean)))\n",
        "char_to_index = {c: i for i, c in enumerate(chars)}\n",
        "index_to_char = {i: c for i, c in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "sequence_length = 80\n",
        "step = 1\n",
        "\n",
        "# convert text to sequences of indices\n",
        "def text_to_sequences(text, sequence_length, step):\n",
        "    text_as_int = [char_to_index[c] for c in text]\n",
        "    inputs, labels = [], []\n",
        "    for i in range(0, len(text_as_int) - sequence_length, step):\n",
        "        inputs.append(text_as_int[i: i + sequence_length])\n",
        "        labels.append(text_as_int[i + 1: i + sequence_length + 1])\n",
        "    return inputs, labels\n",
        "\n",
        "train_sequences, train_labels = text_to_sequences(nietzsche_train_clean, sequence_length, step)\n",
        "test_sequences, test_labels = text_to_sequences(nietzsche_test_clean, sequence_length, step)\n",
        "\n",
        "#train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
        "#test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\n",
        "#\n",
        "## define one-hot encoding function for TPU\n",
        "#def one_hot_encode(x, y):\n",
        "#    x = tf.convert_to_tensor(x)\n",
        "#    y = tf.one_hot(y, depth=vocab_size)  # TPU-friendly one-hot encoding\n",
        "#    return x, y\n",
        "#\n",
        "#\n",
        "## batch, shuffle, and optimize dataset\n",
        "#batch_size = 2048  # TPUs handle larger batch sizes more effectively\n",
        "#train_data = (train_data\n",
        "#              .shuffle(10000)\n",
        "#              .batch(batch_size, drop_remainder=True)\n",
        "#              .map(one_hot_encode)\n",
        "#              .prefetch(tf.data.AUTOTUNE))\n",
        "#test_data = (test_data\n",
        "#             .batch(batch_size, drop_remainder=True)\n",
        "#             .map(one_hot_encode)\n",
        "#             .prefetch(tf.data.AUTOTUNE))\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save character mappings\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/char_mappings.pkl\", \"wb\") as f:\n",
        "    pickle.dump({'char_to_index': char_to_index, 'index_to_char': index_to_char, 'vocab_size': vocab_size}, f)\n",
        "\n",
        "# Save clean training and testing data\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/nietzsche_train_clean.txt\", \"w\") as f:\n",
        "    f.write(nietzsche_train_clean)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/nietzsche_test_clean.txt\", \"w\") as f:\n",
        "    f.write(nietzsche_test_clean)\n",
        "\n",
        "# Save processed train and test sequences\n",
        "train_sequences_np = np.array(train_sequences, dtype=np.int32)\n",
        "train_labels_np = np.array(train_labels, dtype=np.int32)\n",
        "test_sequences_np = np.array(test_sequences, dtype=np.int32)\n",
        "test_labels_np = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/autocomplete/data/train_sequences.npy\", train_sequences_np)\n",
        "np.save(\"/content/drive/MyDrive/autocomplete/data/train_labels.npy\", train_labels_np)\n",
        "np.save(\"/content/drive/MyDrive/autocomplete/data/test_sequences.npy\", test_sequences_np)\n",
        "np.save(\"/content/drive/MyDrive/autocomplete/data/test_labels.npy\", test_labels_np)\n",
        "\n",
        "# Save train and test tf.data Datasets for later loading\n",
        "# train_data.element_spec, test_data.element_spec  # Confirm element specs\n",
        "\n",
        "# You can't directly save tf.data.Dataset but can recreate it later using saved sequences and labels.\n"
      ],
      "metadata": {
        "id": "cBmCnZVJqyTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_sequences_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyC82IPPNLi",
        "outputId": "257755a1-1523-4284-84ed-7cbceb892978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf83PrQRh8rQ",
        "outputId": "1bc42fa5-c7fd-4c0d-a6b1-6960569fa032"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 23, 25, 12, 13, 8, 10, 12, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_sequences[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8qZgJk7iEyn",
        "outputId": "28318f05-3fdb-49c2-a875-3e453174012a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[23, 25, 12, 13, 8, 10, 12, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_labels[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRcPH8O6iIOA",
        "outputId": "af6b2075-f1ea-49d6-dd26-252860e0d8b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48, 47, 0, 53, 41, 38, 0, 36, 41, 54]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_sequences[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aun6f2mKiKwr",
        "outputId": "e8999846-5801-4b84-d486-bb826177515c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[47, 0, 53, 41, 38, 0, 36, 41, 54, 51]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "test_labels[0][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-26yIVmiPI1",
        "outputId": "2e9e0e2b-b767-4fb9-c77c-d0268a6a213e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(train_sequences) # 1180569\n",
        "len(train_sequences[0]) # 80"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfb5xbmsySvNj6A5rqXQE6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}