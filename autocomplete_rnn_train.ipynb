{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JcvEz43u3Z8"
      },
      "source": [
        "# Nietzsche Next\n",
        "- description: Autocomplete app trained on Nietzsche's works. Goal is to make autocompletion proposals for the next word based on the wider context and the characters typed in so far. Uses RNN for character-based prediction (sequence-to-sequence). Updated for TPU usage, parallel data processing, and optimized training on Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_6QE_WUylsd",
        "outputId": "1325e560-70a6-4a67-84bc-61403773cba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU successfully initialized.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"TPU successfully initialized.\")\n",
        "except ValueError:\n",
        "    print(\"TPU not found. Check runtime settings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0asFbfJpulKM",
        "outputId": "695d1306-6560-4f97-a491-6c1df0fd8d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# load character mappings\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/char_mappings.pkl\", \"rb\") as f:\n",
        "    mappings = pickle.load(f)\n",
        "\n",
        "char_to_index = mappings['char_to_index']\n",
        "index_to_char = mappings['index_to_char']\n",
        "vocab_size = mappings['vocab_size']\n",
        "\n",
        "# load cleaned training and testing data\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/nietzsche_train_clean.txt\", \"r\") as f:\n",
        "    nietzsche_train_clean = f.read()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/autocomplete/data/nietzsche_test_clean.txt\", \"r\") as f:\n",
        "    nietzsche_test_clean = f.read()\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Sample training data: {nietzsche_train_clean[:100]}\")\n",
        "print(f\"Sample testing data: {nietzsche_test_clean[:100]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrgDMaTWQDd7",
        "outputId": "d7653d9d-fc5c-4afe-a4fb-9b8b81226033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 60\n",
            "Sample training data:  PREFACE   This book belongs to the most rare of men. Perhaps not one of them is yet alive. It is po\n",
            "Sample testing data: on the church borrowed the fact from Paul.--The God that Paul invented for himself, a God who \"reduc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# load numpy arrays\n",
        "train_sequences_np = np.load(\"/content/drive/MyDrive/autocomplete/data/train_sequences.npy\")\n",
        "train_labels_np = np.load(\"/content/drive/MyDrive/autocomplete/data/train_labels.npy\")\n",
        "test_sequences_np = np.load(\"/content/drive/MyDrive/autocomplete/data/test_sequences.npy\")\n",
        "test_labels_np = np.load(\"/content/drive/MyDrive/autocomplete/data/test_labels.npy\")\n",
        "\n",
        "# convert numpy arrays back to lists\n",
        "train_sequences = train_sequences_np.tolist()\n",
        "train_labels = train_labels_np.tolist()\n",
        "test_sequences = test_sequences_np.tolist()\n",
        "test_labels = test_labels_np.tolist()\n",
        "\n",
        "print(f\"First training sequence: {train_sequences[0]}\")\n",
        "print(f\"First training label: {train_labels[0]}\")\n",
        "print(f\"First testing sequence: {test_sequences[0]}\")\n",
        "print(f\"First testing label: {test_labels[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccfnJucdRgXq",
        "outputId": "1b45355a-8c6f-401d-d259-8a5b1302155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First training sequence: [0, 23, 25, 12, 13, 8, 10, 12, 0, 0, 0, 27, 41, 42, 52, 0, 35, 48, 48, 44, 0, 35, 38, 45, 48, 47, 40, 52, 0, 53, 48, 0, 53, 41, 38, 0, 46, 48, 52, 53, 0, 51, 34, 51, 38, 0, 48, 39, 0, 46, 38, 47, 6, 0, 23, 38, 51, 41, 34, 49, 52, 0, 47, 48, 53, 0, 48, 47, 38, 0, 48, 39, 0, 53, 41, 38, 46, 0, 42, 52]\n",
            "First training label: [23, 25, 12, 13, 8, 10, 12, 0, 0, 0, 27, 41, 42, 52, 0, 35, 48, 48, 44, 0, 35, 38, 45, 48, 47, 40, 52, 0, 53, 48, 0, 53, 41, 38, 0, 46, 48, 52, 53, 0, 51, 34, 51, 38, 0, 48, 39, 0, 46, 38, 47, 6, 0, 23, 38, 51, 41, 34, 49, 52, 0, 47, 48, 53, 0, 48, 47, 38, 0, 48, 39, 0, 53, 41, 38, 46, 0, 42, 52, 0]\n",
            "First testing sequence: [48, 47, 0, 53, 41, 38, 0, 36, 41, 54, 51, 36, 41, 0, 35, 48, 51, 51, 48, 56, 38, 37, 0, 53, 41, 38, 0, 39, 34, 36, 53, 0, 39, 51, 48, 46, 0, 23, 34, 54, 45, 6, 5, 5, 27, 41, 38, 0, 14, 48, 37, 0, 53, 41, 34, 53, 0, 23, 34, 54, 45, 0, 42, 47, 55, 38, 47, 53, 38, 37, 0, 39, 48, 51, 0, 41, 42, 46, 52, 38]\n",
            "First testing label: [47, 0, 53, 41, 38, 0, 36, 41, 54, 51, 36, 41, 0, 35, 48, 51, 51, 48, 56, 38, 37, 0, 53, 41, 38, 0, 39, 34, 36, 53, 0, 39, 51, 48, 46, 0, 23, 34, 54, 45, 6, 5, 5, 27, 41, 38, 0, 14, 48, 37, 0, 53, 41, 34, 53, 0, 23, 34, 54, 45, 0, 42, 47, 55, 38, 47, 53, 38, 37, 0, 39, 48, 51, 0, 41, 42, 46, 52, 38, 45]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByvTnRibu5X1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "sequence_length = 80\n",
        "step = 1\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\n",
        "\n",
        "# define one-hot encoding function for TPU\n",
        "def one_hot_encode(x, y):\n",
        "    x = tf.convert_to_tensor(x)\n",
        "    y = tf.one_hot(y, depth=vocab_size)  # TPU-friendly one-hot encoding\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# batch, shuffle, and optimize dataset\n",
        "batch_size = 2048  # TPUs handle larger batch sizes more effectively\n",
        "train_data = (train_data\n",
        "              .shuffle(10000)\n",
        "              .batch(batch_size, drop_remainder=True)\n",
        "              .map(one_hot_encode)\n",
        "              .prefetch(tf.data.AUTOTUNE))\n",
        "test_data = (test_data\n",
        "             .batch(batch_size, drop_remainder=True)\n",
        "             .map(one_hot_encode)\n",
        "             .prefetch(tf.data.AUTOTUNE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXy5MKKGv_8Y",
        "outputId": "0833a915-1d22-420a-aaa7-b2e71785cccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "576/576 [==============================] - 41s 54ms/step - loss: 3.1220 - accuracy: 0.1689 - val_loss: 3.0324 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 2/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 3.0187 - accuracy: 0.1692 - val_loss: 3.0218 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 3/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 3.0125 - accuracy: 0.1692 - val_loss: 3.0178 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 4/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 3.0096 - accuracy: 0.1692 - val_loss: 3.0154 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 5/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0079 - accuracy: 0.1692 - val_loss: 3.0139 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 6/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0068 - accuracy: 0.1692 - val_loss: 3.0129 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 7/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0060 - accuracy: 0.1692 - val_loss: 3.0120 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 8/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0054 - accuracy: 0.1692 - val_loss: 3.0115 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 9/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0050 - accuracy: 0.1692 - val_loss: 3.0109 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 10/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 3.0046 - accuracy: 0.1692 - val_loss: 3.0104 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 11/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 3.0043 - accuracy: 0.1692 - val_loss: 3.0100 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 12/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 3.0041 - accuracy: 0.1692 - val_loss: 3.0096 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 13/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 3.0037 - accuracy: 0.1692 - val_loss: 3.0092 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 14/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 3.0031 - accuracy: 0.1692 - val_loss: 3.0079 - val_accuracy: 0.1714 - lr: 1.0000e-04\n",
            "Epoch 15/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.9975 - accuracy: 0.1691 - val_loss: 2.9926 - val_accuracy: 0.1692 - lr: 1.0000e-04\n",
            "Epoch 16/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.9318 - accuracy: 0.1869 - val_loss: 2.8672 - val_accuracy: 0.2132 - lr: 1.0000e-04\n",
            "Epoch 17/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.8150 - accuracy: 0.2185 - val_loss: 2.7953 - val_accuracy: 0.2313 - lr: 1.0000e-04\n",
            "Epoch 18/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.7651 - accuracy: 0.2271 - val_loss: 2.7578 - val_accuracy: 0.2364 - lr: 1.0000e-04\n",
            "Epoch 19/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.7323 - accuracy: 0.2327 - val_loss: 2.7293 - val_accuracy: 0.2411 - lr: 1.0000e-04\n",
            "Epoch 20/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.7023 - accuracy: 0.2425 - val_loss: 2.6930 - val_accuracy: 0.2572 - lr: 1.0000e-04\n",
            "Epoch 21/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 2.6572 - accuracy: 0.2574 - val_loss: 2.6293 - val_accuracy: 0.2676 - lr: 1.0000e-04\n",
            "Epoch 22/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.5864 - accuracy: 0.2703 - val_loss: 2.5510 - val_accuracy: 0.2797 - lr: 1.0000e-04\n",
            "Epoch 23/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.5094 - accuracy: 0.2826 - val_loss: 2.4738 - val_accuracy: 0.2920 - lr: 1.0000e-04\n",
            "Epoch 24/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.4485 - accuracy: 0.2944 - val_loss: 2.4204 - val_accuracy: 0.3038 - lr: 1.0000e-04\n",
            "Epoch 25/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.4026 - accuracy: 0.3048 - val_loss: 2.3830 - val_accuracy: 0.3111 - lr: 1.0000e-04\n",
            "Epoch 26/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.3639 - accuracy: 0.3173 - val_loss: 2.3395 - val_accuracy: 0.3313 - lr: 1.0000e-04\n",
            "Epoch 27/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 2.3320 - accuracy: 0.3277 - val_loss: 2.3105 - val_accuracy: 0.3391 - lr: 1.0000e-04\n",
            "Epoch 28/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.3033 - accuracy: 0.3356 - val_loss: 2.2828 - val_accuracy: 0.3459 - lr: 1.0000e-04\n",
            "Epoch 29/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.2774 - accuracy: 0.3419 - val_loss: 2.2563 - val_accuracy: 0.3524 - lr: 1.0000e-04\n",
            "Epoch 30/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.2529 - accuracy: 0.3472 - val_loss: 2.2314 - val_accuracy: 0.3587 - lr: 1.0000e-04\n",
            "Epoch 31/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.2295 - accuracy: 0.3520 - val_loss: 2.2071 - val_accuracy: 0.3629 - lr: 1.0000e-04\n",
            "Epoch 32/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 2.2071 - accuracy: 0.3567 - val_loss: 2.1848 - val_accuracy: 0.3667 - lr: 1.0000e-04\n",
            "Epoch 33/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.1857 - accuracy: 0.3613 - val_loss: 2.1642 - val_accuracy: 0.3717 - lr: 1.0000e-04\n",
            "Epoch 34/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.1655 - accuracy: 0.3660 - val_loss: 2.1432 - val_accuracy: 0.3751 - lr: 1.0000e-04\n",
            "Epoch 35/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.1464 - accuracy: 0.3712 - val_loss: 2.1238 - val_accuracy: 0.3808 - lr: 1.0000e-04\n",
            "Epoch 36/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.1281 - accuracy: 0.3765 - val_loss: 2.1044 - val_accuracy: 0.3875 - lr: 1.0000e-04\n",
            "Epoch 37/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.1106 - accuracy: 0.3815 - val_loss: 2.0890 - val_accuracy: 0.3908 - lr: 1.0000e-04\n",
            "Epoch 38/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.0937 - accuracy: 0.3864 - val_loss: 2.0699 - val_accuracy: 0.3965 - lr: 1.0000e-04\n",
            "Epoch 39/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.0774 - accuracy: 0.3915 - val_loss: 2.0544 - val_accuracy: 0.4024 - lr: 1.0000e-04\n",
            "Epoch 40/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.0617 - accuracy: 0.3962 - val_loss: 2.0391 - val_accuracy: 0.4082 - lr: 1.0000e-04\n",
            "Epoch 41/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 2.0467 - accuracy: 0.4009 - val_loss: 2.0230 - val_accuracy: 0.4133 - lr: 1.0000e-04\n",
            "Epoch 42/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 2.0320 - accuracy: 0.4052 - val_loss: 2.0087 - val_accuracy: 0.4168 - lr: 1.0000e-04\n",
            "Epoch 43/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.0178 - accuracy: 0.4092 - val_loss: 1.9940 - val_accuracy: 0.4202 - lr: 1.0000e-04\n",
            "Epoch 44/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 2.0040 - accuracy: 0.4130 - val_loss: 1.9808 - val_accuracy: 0.4236 - lr: 1.0000e-04\n",
            "Epoch 45/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.9906 - accuracy: 0.4166 - val_loss: 1.9677 - val_accuracy: 0.4268 - lr: 1.0000e-04\n",
            "Epoch 46/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.9776 - accuracy: 0.4200 - val_loss: 1.9558 - val_accuracy: 0.4294 - lr: 1.0000e-04\n",
            "Epoch 47/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.9649 - accuracy: 0.4234 - val_loss: 1.9423 - val_accuracy: 0.4333 - lr: 1.0000e-04\n",
            "Epoch 48/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.9525 - accuracy: 0.4266 - val_loss: 1.9287 - val_accuracy: 0.4372 - lr: 1.0000e-04\n",
            "Epoch 49/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.9404 - accuracy: 0.4298 - val_loss: 1.9164 - val_accuracy: 0.4404 - lr: 1.0000e-04\n",
            "Epoch 50/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.9287 - accuracy: 0.4329 - val_loss: 1.9054 - val_accuracy: 0.4431 - lr: 1.0000e-04\n",
            "Epoch 51/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.9172 - accuracy: 0.4360 - val_loss: 1.8950 - val_accuracy: 0.4456 - lr: 1.0000e-04\n",
            "Epoch 52/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.9061 - accuracy: 0.4390 - val_loss: 1.8826 - val_accuracy: 0.4490 - lr: 1.0000e-04\n",
            "Epoch 53/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.8951 - accuracy: 0.4419 - val_loss: 1.8719 - val_accuracy: 0.4520 - lr: 1.0000e-04\n",
            "Epoch 54/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.8845 - accuracy: 0.4447 - val_loss: 1.8620 - val_accuracy: 0.4543 - lr: 1.0000e-04\n",
            "Epoch 55/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.8741 - accuracy: 0.4476 - val_loss: 1.8512 - val_accuracy: 0.4578 - lr: 1.0000e-04\n",
            "Epoch 56/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.8640 - accuracy: 0.4503 - val_loss: 1.8443 - val_accuracy: 0.4580 - lr: 1.0000e-04\n",
            "Epoch 57/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.8541 - accuracy: 0.4531 - val_loss: 1.8315 - val_accuracy: 0.4639 - lr: 1.0000e-04\n",
            "Epoch 58/250\n",
            "576/576 [==============================] - 33s 57ms/step - loss: 1.8445 - accuracy: 0.4558 - val_loss: 1.8263 - val_accuracy: 0.4636 - lr: 1.0000e-04\n",
            "Epoch 59/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.8352 - accuracy: 0.4584 - val_loss: 1.8132 - val_accuracy: 0.4688 - lr: 1.0000e-04\n",
            "Epoch 60/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.8261 - accuracy: 0.4610 - val_loss: 1.8053 - val_accuracy: 0.4702 - lr: 1.0000e-04\n",
            "Epoch 61/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.8172 - accuracy: 0.4635 - val_loss: 1.7959 - val_accuracy: 0.4731 - lr: 1.0000e-04\n",
            "Epoch 62/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.8086 - accuracy: 0.4658 - val_loss: 1.7892 - val_accuracy: 0.4741 - lr: 1.0000e-04\n",
            "Epoch 63/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.8002 - accuracy: 0.4681 - val_loss: 1.7812 - val_accuracy: 0.4765 - lr: 1.0000e-04\n",
            "Epoch 64/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7920 - accuracy: 0.4704 - val_loss: 1.7745 - val_accuracy: 0.4782 - lr: 1.0000e-04\n",
            "Epoch 65/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7839 - accuracy: 0.4726 - val_loss: 1.7644 - val_accuracy: 0.4816 - lr: 1.0000e-04\n",
            "Epoch 66/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.7761 - accuracy: 0.4748 - val_loss: 1.7580 - val_accuracy: 0.4824 - lr: 1.0000e-04\n",
            "Epoch 67/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7686 - accuracy: 0.4769 - val_loss: 1.7516 - val_accuracy: 0.4843 - lr: 1.0000e-04\n",
            "Epoch 68/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7610 - accuracy: 0.4790 - val_loss: 1.7426 - val_accuracy: 0.4874 - lr: 1.0000e-04\n",
            "Epoch 69/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7537 - accuracy: 0.4810 - val_loss: 1.7355 - val_accuracy: 0.4893 - lr: 1.0000e-04\n",
            "Epoch 70/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.7465 - accuracy: 0.4830 - val_loss: 1.7281 - val_accuracy: 0.4916 - lr: 1.0000e-04\n",
            "Epoch 71/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7395 - accuracy: 0.4850 - val_loss: 1.7219 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
            "Epoch 72/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.7327 - accuracy: 0.4869 - val_loss: 1.7154 - val_accuracy: 0.4949 - lr: 1.0000e-04\n",
            "Epoch 73/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.7260 - accuracy: 0.4888 - val_loss: 1.7084 - val_accuracy: 0.4967 - lr: 1.0000e-04\n",
            "Epoch 74/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.7194 - accuracy: 0.4907 - val_loss: 1.7023 - val_accuracy: 0.4985 - lr: 1.0000e-04\n",
            "Epoch 75/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.7130 - accuracy: 0.4925 - val_loss: 1.6974 - val_accuracy: 0.4997 - lr: 1.0000e-04\n",
            "Epoch 76/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7067 - accuracy: 0.4943 - val_loss: 1.6903 - val_accuracy: 0.5022 - lr: 1.0000e-04\n",
            "Epoch 77/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.7006 - accuracy: 0.4960 - val_loss: 1.6858 - val_accuracy: 0.5033 - lr: 1.0000e-04\n",
            "Epoch 78/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6945 - accuracy: 0.4978 - val_loss: 1.6790 - val_accuracy: 0.5056 - lr: 1.0000e-04\n",
            "Epoch 79/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6887 - accuracy: 0.4994 - val_loss: 1.6735 - val_accuracy: 0.5072 - lr: 1.0000e-04\n",
            "Epoch 80/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6829 - accuracy: 0.5010 - val_loss: 1.6676 - val_accuracy: 0.5089 - lr: 1.0000e-04\n",
            "Epoch 81/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6773 - accuracy: 0.5026 - val_loss: 1.6625 - val_accuracy: 0.5104 - lr: 1.0000e-04\n",
            "Epoch 82/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.6718 - accuracy: 0.5042 - val_loss: 1.6571 - val_accuracy: 0.5119 - lr: 1.0000e-04\n",
            "Epoch 83/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6664 - accuracy: 0.5058 - val_loss: 1.6522 - val_accuracy: 0.5131 - lr: 1.0000e-04\n",
            "Epoch 84/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.6611 - accuracy: 0.5072 - val_loss: 1.6472 - val_accuracy: 0.5146 - lr: 1.0000e-04\n",
            "Epoch 85/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6560 - accuracy: 0.5086 - val_loss: 1.6426 - val_accuracy: 0.5158 - lr: 1.0000e-04\n",
            "Epoch 86/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6510 - accuracy: 0.5100 - val_loss: 1.6379 - val_accuracy: 0.5172 - lr: 1.0000e-04\n",
            "Epoch 87/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6460 - accuracy: 0.5114 - val_loss: 1.6334 - val_accuracy: 0.5184 - lr: 1.0000e-04\n",
            "Epoch 88/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6412 - accuracy: 0.5127 - val_loss: 1.6307 - val_accuracy: 0.5191 - lr: 1.0000e-04\n",
            "Epoch 89/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6365 - accuracy: 0.5140 - val_loss: 1.6245 - val_accuracy: 0.5210 - lr: 1.0000e-04\n",
            "Epoch 90/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.6319 - accuracy: 0.5152 - val_loss: 1.6218 - val_accuracy: 0.5220 - lr: 1.0000e-04\n",
            "Epoch 91/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.6274 - accuracy: 0.5165 - val_loss: 1.6158 - val_accuracy: 0.5235 - lr: 1.0000e-04\n",
            "Epoch 92/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6229 - accuracy: 0.5177 - val_loss: 1.6118 - val_accuracy: 0.5247 - lr: 1.0000e-04\n",
            "Epoch 93/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6186 - accuracy: 0.5188 - val_loss: 1.6101 - val_accuracy: 0.5254 - lr: 1.0000e-04\n",
            "Epoch 94/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6143 - accuracy: 0.5200 - val_loss: 1.6043 - val_accuracy: 0.5268 - lr: 1.0000e-04\n",
            "Epoch 95/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6101 - accuracy: 0.5211 - val_loss: 1.5998 - val_accuracy: 0.5280 - lr: 1.0000e-04\n",
            "Epoch 96/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.6061 - accuracy: 0.5221 - val_loss: 1.5981 - val_accuracy: 0.5285 - lr: 1.0000e-04\n",
            "Epoch 97/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.6020 - accuracy: 0.5232 - val_loss: 1.5928 - val_accuracy: 0.5298 - lr: 1.0000e-04\n",
            "Epoch 98/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5981 - accuracy: 0.5243 - val_loss: 1.5887 - val_accuracy: 0.5309 - lr: 1.0000e-04\n",
            "Epoch 99/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5942 - accuracy: 0.5253 - val_loss: 1.5855 - val_accuracy: 0.5316 - lr: 1.0000e-04\n",
            "Epoch 100/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.5903 - accuracy: 0.5264 - val_loss: 1.5842 - val_accuracy: 0.5320 - lr: 1.0000e-04\n",
            "Epoch 101/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5866 - accuracy: 0.5274 - val_loss: 1.5785 - val_accuracy: 0.5333 - lr: 1.0000e-04\n",
            "Epoch 102/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5829 - accuracy: 0.5284 - val_loss: 1.5749 - val_accuracy: 0.5344 - lr: 1.0000e-04\n",
            "Epoch 103/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.5794 - accuracy: 0.5293 - val_loss: 1.5726 - val_accuracy: 0.5350 - lr: 1.0000e-04\n",
            "Epoch 104/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.5758 - accuracy: 0.5303 - val_loss: 1.5696 - val_accuracy: 0.5355 - lr: 1.0000e-04\n",
            "Epoch 105/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5723 - accuracy: 0.5313 - val_loss: 1.5657 - val_accuracy: 0.5365 - lr: 1.0000e-04\n",
            "Epoch 106/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5688 - accuracy: 0.5322 - val_loss: 1.5617 - val_accuracy: 0.5376 - lr: 1.0000e-04\n",
            "Epoch 107/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5655 - accuracy: 0.5332 - val_loss: 1.5586 - val_accuracy: 0.5384 - lr: 1.0000e-04\n",
            "Epoch 108/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5621 - accuracy: 0.5341 - val_loss: 1.5573 - val_accuracy: 0.5388 - lr: 1.0000e-04\n",
            "Epoch 109/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5588 - accuracy: 0.5350 - val_loss: 1.5527 - val_accuracy: 0.5400 - lr: 1.0000e-04\n",
            "Epoch 110/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5555 - accuracy: 0.5359 - val_loss: 1.5514 - val_accuracy: 0.5405 - lr: 1.0000e-04\n",
            "Epoch 111/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5524 - accuracy: 0.5367 - val_loss: 1.5470 - val_accuracy: 0.5413 - lr: 1.0000e-04\n",
            "Epoch 112/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5492 - accuracy: 0.5376 - val_loss: 1.5438 - val_accuracy: 0.5423 - lr: 1.0000e-04\n",
            "Epoch 113/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5461 - accuracy: 0.5384 - val_loss: 1.5409 - val_accuracy: 0.5430 - lr: 1.0000e-04\n",
            "Epoch 114/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5431 - accuracy: 0.5392 - val_loss: 1.5401 - val_accuracy: 0.5434 - lr: 1.0000e-04\n",
            "Epoch 115/250\n",
            "576/576 [==============================] - 32s 56ms/step - loss: 1.5401 - accuracy: 0.5401 - val_loss: 1.5355 - val_accuracy: 0.5445 - lr: 1.0000e-04\n",
            "Epoch 116/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5371 - accuracy: 0.5408 - val_loss: 1.5326 - val_accuracy: 0.5453 - lr: 1.0000e-04\n",
            "Epoch 117/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5343 - accuracy: 0.5416 - val_loss: 1.5301 - val_accuracy: 0.5459 - lr: 1.0000e-04\n",
            "Epoch 118/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5314 - accuracy: 0.5424 - val_loss: 1.5275 - val_accuracy: 0.5467 - lr: 1.0000e-04\n",
            "Epoch 119/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5287 - accuracy: 0.5431 - val_loss: 1.5249 - val_accuracy: 0.5474 - lr: 1.0000e-04\n",
            "Epoch 120/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5259 - accuracy: 0.5439 - val_loss: 1.5236 - val_accuracy: 0.5477 - lr: 1.0000e-04\n",
            "Epoch 121/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.5231 - accuracy: 0.5447 - val_loss: 1.5198 - val_accuracy: 0.5487 - lr: 1.0000e-04\n",
            "Epoch 122/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5204 - accuracy: 0.5454 - val_loss: 1.5174 - val_accuracy: 0.5490 - lr: 1.0000e-04\n",
            "Epoch 123/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.5177 - accuracy: 0.5461 - val_loss: 1.5166 - val_accuracy: 0.5495 - lr: 1.0000e-04\n",
            "Epoch 124/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5151 - accuracy: 0.5468 - val_loss: 1.5126 - val_accuracy: 0.5504 - lr: 1.0000e-04\n",
            "Epoch 125/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5125 - accuracy: 0.5475 - val_loss: 1.5120 - val_accuracy: 0.5507 - lr: 1.0000e-04\n",
            "Epoch 126/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5100 - accuracy: 0.5482 - val_loss: 1.5078 - val_accuracy: 0.5517 - lr: 1.0000e-04\n",
            "Epoch 127/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.5075 - accuracy: 0.5488 - val_loss: 1.5060 - val_accuracy: 0.5520 - lr: 1.0000e-04\n",
            "Epoch 128/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5050 - accuracy: 0.5495 - val_loss: 1.5042 - val_accuracy: 0.5527 - lr: 1.0000e-04\n",
            "Epoch 129/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5026 - accuracy: 0.5502 - val_loss: 1.5011 - val_accuracy: 0.5533 - lr: 1.0000e-04\n",
            "Epoch 130/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.5001 - accuracy: 0.5509 - val_loss: 1.4989 - val_accuracy: 0.5541 - lr: 1.0000e-04\n",
            "Epoch 131/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4977 - accuracy: 0.5515 - val_loss: 1.4968 - val_accuracy: 0.5546 - lr: 1.0000e-04\n",
            "Epoch 132/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4954 - accuracy: 0.5521 - val_loss: 1.4949 - val_accuracy: 0.5548 - lr: 1.0000e-04\n",
            "Epoch 133/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4931 - accuracy: 0.5528 - val_loss: 1.4936 - val_accuracy: 0.5555 - lr: 1.0000e-04\n",
            "Epoch 134/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4908 - accuracy: 0.5533 - val_loss: 1.4908 - val_accuracy: 0.5559 - lr: 1.0000e-04\n",
            "Epoch 135/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4885 - accuracy: 0.5539 - val_loss: 1.4886 - val_accuracy: 0.5564 - lr: 1.0000e-04\n",
            "Epoch 136/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4863 - accuracy: 0.5545 - val_loss: 1.4867 - val_accuracy: 0.5570 - lr: 1.0000e-04\n",
            "Epoch 137/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4841 - accuracy: 0.5551 - val_loss: 1.4852 - val_accuracy: 0.5573 - lr: 1.0000e-04\n",
            "Epoch 138/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4819 - accuracy: 0.5557 - val_loss: 1.4838 - val_accuracy: 0.5578 - lr: 1.0000e-04\n",
            "Epoch 139/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4798 - accuracy: 0.5563 - val_loss: 1.4809 - val_accuracy: 0.5585 - lr: 1.0000e-04\n",
            "Epoch 140/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4777 - accuracy: 0.5568 - val_loss: 1.4791 - val_accuracy: 0.5590 - lr: 1.0000e-04\n",
            "Epoch 141/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4755 - accuracy: 0.5574 - val_loss: 1.4789 - val_accuracy: 0.5593 - lr: 1.0000e-04\n",
            "Epoch 142/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4734 - accuracy: 0.5579 - val_loss: 1.4754 - val_accuracy: 0.5599 - lr: 1.0000e-04\n",
            "Epoch 143/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4714 - accuracy: 0.5585 - val_loss: 1.4733 - val_accuracy: 0.5606 - lr: 1.0000e-04\n",
            "Epoch 144/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4694 - accuracy: 0.5590 - val_loss: 1.4727 - val_accuracy: 0.5608 - lr: 1.0000e-04\n",
            "Epoch 145/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4673 - accuracy: 0.5595 - val_loss: 1.4701 - val_accuracy: 0.5613 - lr: 1.0000e-04\n",
            "Epoch 146/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4653 - accuracy: 0.5600 - val_loss: 1.4682 - val_accuracy: 0.5619 - lr: 1.0000e-04\n",
            "Epoch 147/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4634 - accuracy: 0.5605 - val_loss: 1.4668 - val_accuracy: 0.5623 - lr: 1.0000e-04\n",
            "Epoch 148/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4613 - accuracy: 0.5611 - val_loss: 1.4654 - val_accuracy: 0.5628 - lr: 1.0000e-04\n",
            "Epoch 149/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4593 - accuracy: 0.5616 - val_loss: 1.4634 - val_accuracy: 0.5631 - lr: 1.0000e-04\n",
            "Epoch 150/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4574 - accuracy: 0.5621 - val_loss: 1.4628 - val_accuracy: 0.5633 - lr: 1.0000e-04\n",
            "Epoch 151/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4555 - accuracy: 0.5626 - val_loss: 1.4615 - val_accuracy: 0.5637 - lr: 1.0000e-04\n",
            "Epoch 152/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4536 - accuracy: 0.5630 - val_loss: 1.4601 - val_accuracy: 0.5639 - lr: 1.0000e-04\n",
            "Epoch 153/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4517 - accuracy: 0.5635 - val_loss: 1.4564 - val_accuracy: 0.5650 - lr: 1.0000e-04\n",
            "Epoch 154/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4499 - accuracy: 0.5640 - val_loss: 1.4566 - val_accuracy: 0.5649 - lr: 1.0000e-04\n",
            "Epoch 155/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4480 - accuracy: 0.5645 - val_loss: 1.4535 - val_accuracy: 0.5658 - lr: 1.0000e-04\n",
            "Epoch 156/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4462 - accuracy: 0.5650 - val_loss: 1.4534 - val_accuracy: 0.5658 - lr: 1.0000e-04\n",
            "Epoch 157/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4445 - accuracy: 0.5655 - val_loss: 1.4514 - val_accuracy: 0.5664 - lr: 1.0000e-04\n",
            "Epoch 158/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4427 - accuracy: 0.5660 - val_loss: 1.4488 - val_accuracy: 0.5671 - lr: 1.0000e-04\n",
            "Epoch 159/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4410 - accuracy: 0.5664 - val_loss: 1.4480 - val_accuracy: 0.5675 - lr: 1.0000e-04\n",
            "Epoch 160/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4392 - accuracy: 0.5669 - val_loss: 1.4461 - val_accuracy: 0.5678 - lr: 1.0000e-04\n",
            "Epoch 161/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4375 - accuracy: 0.5673 - val_loss: 1.4444 - val_accuracy: 0.5684 - lr: 1.0000e-04\n",
            "Epoch 162/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4359 - accuracy: 0.5678 - val_loss: 1.4442 - val_accuracy: 0.5683 - lr: 1.0000e-04\n",
            "Epoch 163/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4342 - accuracy: 0.5683 - val_loss: 1.4418 - val_accuracy: 0.5691 - lr: 1.0000e-04\n",
            "Epoch 164/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4326 - accuracy: 0.5687 - val_loss: 1.4403 - val_accuracy: 0.5694 - lr: 1.0000e-04\n",
            "Epoch 165/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4310 - accuracy: 0.5691 - val_loss: 1.4392 - val_accuracy: 0.5698 - lr: 1.0000e-04\n",
            "Epoch 166/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4295 - accuracy: 0.5696 - val_loss: 1.4385 - val_accuracy: 0.5696 - lr: 1.0000e-04\n",
            "Epoch 167/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4278 - accuracy: 0.5700 - val_loss: 1.4377 - val_accuracy: 0.5699 - lr: 1.0000e-04\n",
            "Epoch 168/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4262 - accuracy: 0.5705 - val_loss: 1.4348 - val_accuracy: 0.5709 - lr: 1.0000e-04\n",
            "Epoch 169/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4248 - accuracy: 0.5708 - val_loss: 1.4351 - val_accuracy: 0.5706 - lr: 1.0000e-04\n",
            "Epoch 170/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4232 - accuracy: 0.5712 - val_loss: 1.4325 - val_accuracy: 0.5715 - lr: 1.0000e-04\n",
            "Epoch 171/250\n",
            "576/576 [==============================] - 32s 56ms/step - loss: 1.4217 - accuracy: 0.5717 - val_loss: 1.4314 - val_accuracy: 0.5717 - lr: 1.0000e-04\n",
            "Epoch 172/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4202 - accuracy: 0.5721 - val_loss: 1.4309 - val_accuracy: 0.5717 - lr: 1.0000e-04\n",
            "Epoch 173/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4187 - accuracy: 0.5725 - val_loss: 1.4286 - val_accuracy: 0.5725 - lr: 1.0000e-04\n",
            "Epoch 174/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4173 - accuracy: 0.5728 - val_loss: 1.4275 - val_accuracy: 0.5729 - lr: 1.0000e-04\n",
            "Epoch 175/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4159 - accuracy: 0.5733 - val_loss: 1.4271 - val_accuracy: 0.5727 - lr: 1.0000e-04\n",
            "Epoch 176/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4144 - accuracy: 0.5736 - val_loss: 1.4254 - val_accuracy: 0.5733 - lr: 1.0000e-04\n",
            "Epoch 177/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4130 - accuracy: 0.5740 - val_loss: 1.4251 - val_accuracy: 0.5731 - lr: 1.0000e-04\n",
            "Epoch 178/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4116 - accuracy: 0.5743 - val_loss: 1.4230 - val_accuracy: 0.5738 - lr: 1.0000e-04\n",
            "Epoch 179/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4103 - accuracy: 0.5747 - val_loss: 1.4217 - val_accuracy: 0.5742 - lr: 1.0000e-04\n",
            "Epoch 180/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4088 - accuracy: 0.5751 - val_loss: 1.4207 - val_accuracy: 0.5744 - lr: 1.0000e-04\n",
            "Epoch 181/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.4075 - accuracy: 0.5754 - val_loss: 1.4193 - val_accuracy: 0.5748 - lr: 1.0000e-04\n",
            "Epoch 182/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4061 - accuracy: 0.5758 - val_loss: 1.4182 - val_accuracy: 0.5750 - lr: 1.0000e-04\n",
            "Epoch 183/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4048 - accuracy: 0.5762 - val_loss: 1.4174 - val_accuracy: 0.5752 - lr: 1.0000e-04\n",
            "Epoch 184/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.4036 - accuracy: 0.5765 - val_loss: 1.4161 - val_accuracy: 0.5758 - lr: 1.0000e-04\n",
            "Epoch 185/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4022 - accuracy: 0.5769 - val_loss: 1.4166 - val_accuracy: 0.5753 - lr: 1.0000e-04\n",
            "Epoch 186/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.4010 - accuracy: 0.5772 - val_loss: 1.4141 - val_accuracy: 0.5762 - lr: 1.0000e-04\n",
            "Epoch 187/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3996 - accuracy: 0.5776 - val_loss: 1.4131 - val_accuracy: 0.5764 - lr: 1.0000e-04\n",
            "Epoch 188/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3983 - accuracy: 0.5779 - val_loss: 1.4122 - val_accuracy: 0.5767 - lr: 1.0000e-04\n",
            "Epoch 189/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3971 - accuracy: 0.5782 - val_loss: 1.4121 - val_accuracy: 0.5766 - lr: 1.0000e-04\n",
            "Epoch 190/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3958 - accuracy: 0.5785 - val_loss: 1.4104 - val_accuracy: 0.5770 - lr: 1.0000e-04\n",
            "Epoch 191/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3946 - accuracy: 0.5789 - val_loss: 1.4093 - val_accuracy: 0.5774 - lr: 1.0000e-04\n",
            "Epoch 192/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3934 - accuracy: 0.5792 - val_loss: 1.4084 - val_accuracy: 0.5776 - lr: 1.0000e-04\n",
            "Epoch 193/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3922 - accuracy: 0.5795 - val_loss: 1.4083 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
            "Epoch 194/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3909 - accuracy: 0.5798 - val_loss: 1.4058 - val_accuracy: 0.5782 - lr: 1.0000e-04\n",
            "Epoch 195/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3898 - accuracy: 0.5802 - val_loss: 1.4050 - val_accuracy: 0.5784 - lr: 1.0000e-04\n",
            "Epoch 196/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3886 - accuracy: 0.5805 - val_loss: 1.4050 - val_accuracy: 0.5785 - lr: 1.0000e-04\n",
            "Epoch 197/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3874 - accuracy: 0.5808 - val_loss: 1.4040 - val_accuracy: 0.5786 - lr: 1.0000e-04\n",
            "Epoch 198/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3863 - accuracy: 0.5811 - val_loss: 1.4022 - val_accuracy: 0.5791 - lr: 1.0000e-04\n",
            "Epoch 199/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3851 - accuracy: 0.5814 - val_loss: 1.4019 - val_accuracy: 0.5791 - lr: 1.0000e-04\n",
            "Epoch 200/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3840 - accuracy: 0.5817 - val_loss: 1.4003 - val_accuracy: 0.5796 - lr: 1.0000e-04\n",
            "Epoch 201/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3829 - accuracy: 0.5820 - val_loss: 1.3995 - val_accuracy: 0.5798 - lr: 1.0000e-04\n",
            "Epoch 202/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3817 - accuracy: 0.5823 - val_loss: 1.3986 - val_accuracy: 0.5800 - lr: 1.0000e-04\n",
            "Epoch 203/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3805 - accuracy: 0.5827 - val_loss: 1.3981 - val_accuracy: 0.5802 - lr: 1.0000e-04\n",
            "Epoch 204/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3794 - accuracy: 0.5829 - val_loss: 1.3977 - val_accuracy: 0.5803 - lr: 1.0000e-04\n",
            "Epoch 205/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3783 - accuracy: 0.5832 - val_loss: 1.3967 - val_accuracy: 0.5805 - lr: 1.0000e-04\n",
            "Epoch 206/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3773 - accuracy: 0.5835 - val_loss: 1.3967 - val_accuracy: 0.5806 - lr: 1.0000e-04\n",
            "Epoch 207/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3762 - accuracy: 0.5838 - val_loss: 1.3941 - val_accuracy: 0.5813 - lr: 1.0000e-04\n",
            "Epoch 208/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3752 - accuracy: 0.5841 - val_loss: 1.3932 - val_accuracy: 0.5815 - lr: 1.0000e-04\n",
            "Epoch 209/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3741 - accuracy: 0.5843 - val_loss: 1.3933 - val_accuracy: 0.5816 - lr: 1.0000e-04\n",
            "Epoch 210/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3730 - accuracy: 0.5846 - val_loss: 1.3926 - val_accuracy: 0.5817 - lr: 1.0000e-04\n",
            "Epoch 211/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3720 - accuracy: 0.5849 - val_loss: 1.3906 - val_accuracy: 0.5822 - lr: 1.0000e-04\n",
            "Epoch 212/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3709 - accuracy: 0.5852 - val_loss: 1.3897 - val_accuracy: 0.5824 - lr: 1.0000e-04\n",
            "Epoch 213/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3699 - accuracy: 0.5854 - val_loss: 1.3890 - val_accuracy: 0.5827 - lr: 1.0000e-04\n",
            "Epoch 214/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3690 - accuracy: 0.5857 - val_loss: 1.3882 - val_accuracy: 0.5829 - lr: 1.0000e-04\n",
            "Epoch 215/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3679 - accuracy: 0.5860 - val_loss: 1.3874 - val_accuracy: 0.5830 - lr: 1.0000e-04\n",
            "Epoch 216/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3669 - accuracy: 0.5863 - val_loss: 1.3867 - val_accuracy: 0.5832 - lr: 1.0000e-04\n",
            "Epoch 217/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3658 - accuracy: 0.5866 - val_loss: 1.3864 - val_accuracy: 0.5833 - lr: 1.0000e-04\n",
            "Epoch 218/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3649 - accuracy: 0.5868 - val_loss: 1.3861 - val_accuracy: 0.5833 - lr: 1.0000e-04\n",
            "Epoch 219/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3639 - accuracy: 0.5870 - val_loss: 1.3843 - val_accuracy: 0.5838 - lr: 1.0000e-04\n",
            "Epoch 220/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3630 - accuracy: 0.5873 - val_loss: 1.3833 - val_accuracy: 0.5841 - lr: 1.0000e-04\n",
            "Epoch 221/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3620 - accuracy: 0.5876 - val_loss: 1.3826 - val_accuracy: 0.5843 - lr: 1.0000e-04\n",
            "Epoch 222/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3610 - accuracy: 0.5879 - val_loss: 1.3816 - val_accuracy: 0.5846 - lr: 1.0000e-04\n",
            "Epoch 223/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3600 - accuracy: 0.5880 - val_loss: 1.3817 - val_accuracy: 0.5844 - lr: 1.0000e-04\n",
            "Epoch 224/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3591 - accuracy: 0.5883 - val_loss: 1.3812 - val_accuracy: 0.5846 - lr: 1.0000e-04\n",
            "Epoch 225/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3582 - accuracy: 0.5885 - val_loss: 1.3796 - val_accuracy: 0.5850 - lr: 1.0000e-04\n",
            "Epoch 226/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3572 - accuracy: 0.5888 - val_loss: 1.3786 - val_accuracy: 0.5852 - lr: 1.0000e-04\n",
            "Epoch 227/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3563 - accuracy: 0.5891 - val_loss: 1.3792 - val_accuracy: 0.5852 - lr: 1.0000e-04\n",
            "Epoch 228/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3554 - accuracy: 0.5893 - val_loss: 1.3775 - val_accuracy: 0.5855 - lr: 1.0000e-04\n",
            "Epoch 229/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3545 - accuracy: 0.5895 - val_loss: 1.3768 - val_accuracy: 0.5856 - lr: 1.0000e-04\n",
            "Epoch 230/250\n",
            "576/576 [==============================] - 29s 49ms/step - loss: 1.3536 - accuracy: 0.5897 - val_loss: 1.3768 - val_accuracy: 0.5858 - lr: 1.0000e-04\n",
            "Epoch 231/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3527 - accuracy: 0.5900 - val_loss: 1.3755 - val_accuracy: 0.5860 - lr: 1.0000e-04\n",
            "Epoch 232/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3518 - accuracy: 0.5902 - val_loss: 1.3745 - val_accuracy: 0.5864 - lr: 1.0000e-04\n",
            "Epoch 233/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3510 - accuracy: 0.5904 - val_loss: 1.3751 - val_accuracy: 0.5862 - lr: 1.0000e-04\n",
            "Epoch 234/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3501 - accuracy: 0.5907 - val_loss: 1.3732 - val_accuracy: 0.5866 - lr: 1.0000e-04\n",
            "Epoch 235/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3492 - accuracy: 0.5909 - val_loss: 1.3729 - val_accuracy: 0.5867 - lr: 1.0000e-04\n",
            "Epoch 236/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3483 - accuracy: 0.5912 - val_loss: 1.3728 - val_accuracy: 0.5869 - lr: 1.0000e-04\n",
            "Epoch 237/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3474 - accuracy: 0.5914 - val_loss: 1.3717 - val_accuracy: 0.5871 - lr: 1.0000e-04\n",
            "Epoch 238/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3466 - accuracy: 0.5916 - val_loss: 1.3709 - val_accuracy: 0.5873 - lr: 1.0000e-04\n",
            "Epoch 239/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3457 - accuracy: 0.5918 - val_loss: 1.3706 - val_accuracy: 0.5875 - lr: 1.0000e-04\n",
            "Epoch 240/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3449 - accuracy: 0.5921 - val_loss: 1.3694 - val_accuracy: 0.5876 - lr: 1.0000e-04\n",
            "Epoch 241/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3441 - accuracy: 0.5922 - val_loss: 1.3697 - val_accuracy: 0.5878 - lr: 1.0000e-04\n",
            "Epoch 242/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3433 - accuracy: 0.5925 - val_loss: 1.3690 - val_accuracy: 0.5880 - lr: 1.0000e-04\n",
            "Epoch 243/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3424 - accuracy: 0.5927 - val_loss: 1.3671 - val_accuracy: 0.5884 - lr: 1.0000e-04\n",
            "Epoch 244/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3415 - accuracy: 0.5929 - val_loss: 1.3667 - val_accuracy: 0.5887 - lr: 1.0000e-04\n",
            "Epoch 245/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3407 - accuracy: 0.5931 - val_loss: 1.3663 - val_accuracy: 0.5886 - lr: 1.0000e-04\n",
            "Epoch 246/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3399 - accuracy: 0.5934 - val_loss: 1.3662 - val_accuracy: 0.5887 - lr: 1.0000e-04\n",
            "Epoch 247/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3391 - accuracy: 0.5936 - val_loss: 1.3662 - val_accuracy: 0.5888 - lr: 1.0000e-04\n",
            "Epoch 248/250\n",
            "576/576 [==============================] - 28s 49ms/step - loss: 1.3383 - accuracy: 0.5938 - val_loss: 1.3644 - val_accuracy: 0.5890 - lr: 1.0000e-04\n",
            "Epoch 249/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3375 - accuracy: 0.5940 - val_loss: 1.3639 - val_accuracy: 0.5894 - lr: 1.0000e-04\n",
            "Epoch 250/250\n",
            "576/576 [==============================] - 29s 50ms/step - loss: 1.3367 - accuracy: 0.5942 - val_loss: 1.3633 - val_accuracy: 0.5894 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 50\n",
        "dropout_rate = 0.1 # .5\n",
        "n_epochs = 250\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "        LSTM(256, return_sequences=True, dropout=dropout_rate),\n",
        "        LSTM(256, return_sequences=True, dropout=dropout_rate),\n",
        "        LSTM(256, return_sequences=True, dropout=dropout_rate),\n",
        "        TimeDistributed(Dense(vocab_size, activation='softmax'))       # Added TimeDistributed layer\n",
        "    ])\n",
        "\n",
        "    optimizer = RMSprop(learning_rate=0.0001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),  # Increased patience\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)  # Added learning rate scheduler\n",
        "    ]\n",
        "\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        validation_data=test_data,\n",
        "        epochs=n_epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q606-VwLU16t"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/content/drive/MyDrive/nietzsche_model_weights_trained_v04_1.h5')\n",
        "model.save('/content/drive/MyDrive/nietzsche_model_v04_1_tf',save_format='tf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h29_DOal0HJ3",
        "outputId": "a7fbf3ab-f3e5-4c72-ef76-652da61098eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                                                          God is the \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def autocomplete_text(model, seed_text, char_to_index, index_to_char, sequence_length=sequence_length, num_predictions=100):\n",
        "    # pad seed text if len(seed text) < sequence length\n",
        "    if len(seed_text) < sequence_length:\n",
        "        seed_text = ' ' * (sequence_length - len(seed_text)) + seed_text\n",
        "\n",
        "    input_sequence = [char_to_index[char] for char in seed_text]\n",
        "\n",
        "    # generate characters one by one\n",
        "    output_text = seed_text\n",
        "    for _ in range(num_predictions):\n",
        "        # reshape to match model input shape (1, sequence_length)\n",
        "        input_array = np.reshape(input_sequence, (1, sequence_length))\n",
        "\n",
        "        predicted_probs = model.predict(input_array, verbose=0)\n",
        "        predicted_char_index = np.argmax(predicted_probs[0, -1])  # Choose the most likely character\n",
        "        predicted_char = index_to_char[predicted_char_index]\n",
        "        output_text += predicted_char\n",
        "\n",
        "        # update input sequence by adding the predicted character and removing the first one\n",
        "        input_sequence.append(predicted_char_index)\n",
        "        input_sequence = input_sequence[1:]\n",
        "\n",
        "        if predicted_char == ' ':\n",
        "            break\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# example\n",
        "seed_text = 'God is '\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbsEePVB0bKZ",
        "outputId": "33556076-16d1-410d-b1e3-e148e85516ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                                                         God is decadence \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'God is d'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gZV5CA68WhB",
        "outputId": "5b8485b7-5a2c-4619-9dd0-6a91bde29cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                                                        God is decadence \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'God is de'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j79PNRK8Y-J",
        "outputId": "94d5ce54-d50c-4382-ee03-31c301dfaa1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                                                       God is death \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'God is dea'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p81DlKSicWgl"
      },
      "outputs": [],
      "source": [
        "# last sentence of \"human\":\n",
        "# In the same manner I have viewed the saints of India who occupy an intermediate station between the christian saints and the Greek philosophers\n",
        "# and hence are not to be regarded as a pure type. Knowledge and science--as far as they existed--and superiority to the rest of mankind by logical\n",
        "# discipline and training of the intellectual powers were insisted upon by the Buddhists as essential to sanctity, just as they were denounced by\n",
        "# the christian world as the indications of sinfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvr1-fBXg9mf",
        "outputId": "4a91bfa1-b35b-40b8-838b-b1ec595954d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                       the christian world as the indications of the \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'the christian world as the indications of '\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIoW6zP4hCV_",
        "outputId": "d32bdcd3-638a-4e10-e576-bce0bb83a7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                      the christian world as the indications of such \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'the christian world as the indications of s'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD6HydDrhKAo",
        "outputId": "db9341e7-1127-49cb-a981-bb43359ea677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                     the christian world as the indications of significance \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'the christian world as the indications of si'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBi0_-01s9KX",
        "outputId": "6b55d270-bc42-476f-e714-893fc9b68c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                   the christian world as the indications of sinful \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'the christian world as the indications of sinf'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_bsD4n_76Y",
        "outputId": "60d15fa9-cba2-4f6a-cd54-248f26499529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocomplete suggestion:                                the christian world as the indications of sinfulness \n"
          ]
        }
      ],
      "source": [
        "seed_text = 'the christian world as the indications of sinfuln'\n",
        "completion = autocomplete_text(model, seed_text, char_to_index, index_to_char)\n",
        "print(\"Autocomplete suggestion:\", completion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu7tDbV48XPCuDmdfuDMMF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}